\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[preprint]{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{algpseudocode}
\usepackage{algorithm}

\title{Theory and Methods for the Ferromagnetic Ising Model}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{
    Jay Shen \\
    Department of Physics \\
    University of Chicago\\
    Chicago, IL 60637 \\
    \texttt{jshe@uchicago.edu} \\
    \And
    Mark Lee \\
    Department of Statistics \\
    University of Chicago\\
    Chicago, IL 60637 \\
    \texttt{markyl@uchicago.edu} \\
}


\begin{document}

\graphicspath{ {../graphics} }

\maketitle

\begin{abstract}

The Ising model is a historically important problem in statistical mechanics 
\cite{lenz1920}. 
Although it was originally proposed as a crude approximation of magnetic 
phenomena, it furnished statistical mechanics, and later probabilistic science 
as whole, with a new paradigm of graph-based modeling that would prove 
influential.
Especially in the distilled, theoretical study of probabilistic graphical 
models (PGMs), which today is its own field, many methods developed for Ising 
models and spin-glasses—as well as the rich physical vocabulary of energies, 
entropy, and partition functions—have been repurposed and reinterpreted for 
application towards diverse problems \cite{koller}. 

In this paper, we pay homage to the Ising model by applying modern computational 
solutions to the original problem of magnetism. 
We discuss the theory of the Ising model from a physical perspective, and 
examine its correspondence with PGMs theory. 
We then evaluate two approaches to inference—Markov Chain Monte Carlo and
belief propagation—and discuss their strengths and weaknesses when appplied to 
predict physical phenomena. 
%
%
%
%
%
\end{abstract}
%
%
%
%
%
\section{Theory of the Ising Model}

The prevailing physical theory explains magnetism as a consequence of the 
intrinsic spin all particles possess \cite{griffiths}. 
This spin produces a magnetic dipole moment $\vec{\mu}$ proportional to the 
spin $\sigma$:
\[\vec{\mu} = \mu \sigma\]
If an external magnetic field $\vec{B}$ is present, the potential energy is 
given by the classical formula:
\[U_B = - \vec{\mu} \cdot \vec{B}\]

There also exist pairwise exchange interactions between particles due to the 
mutual effects of their magnetic moments.
These exchange effects have strengths defined by constants $J_{ij}$. 
The potential energy contained by these pairwise interactions is:
\[U_{ij} = - J_{ij} \vec{\mu_i} \cdot \vec{\mu_j}\]
For simplicity, we will ignore other, more complicated phenomena. 

Now, consider a collection of particles 
$\vec{\mu} = (\vec{\mu}_1, \dots, \vec{\mu_n})$ 
in the prescence of an external magnetic field $\vec{B}$. 
Note that both potential energies are minimized when the moments align with 
$\vec{B}$ and with each other. 
This corresponds to the explanation of ferromagnetism as a systematic alignment 
of spins. 
The Hamiltonian, which specifies the total energy of the system, is defined by 
the sum of all these pairwise and unary energies:
\begin{equation}\label{eq:exactE}
    E(\vec{\mu}) = -\frac{1}{2}\sum_i^n \sum_j^n J_{ij} \vec{\mu_i} \cdot \vec{\mu_j} - \sum_i^n \vec{\mu_i} \cdot \vec{B}
\end{equation}
In most cases, working with this Hamiltonian is intractable. 
Luckily, we can simplify it tremendously by making several assumptions. 

First, in the context of an atomic lattice, all sites are assumed to be 
identical and representable by a single fermionic spin variable. 
That is, each site's magnetic moment simplifies as $\vec{\mu_i} = \mu \sigma_i$, 
where the spin $\sigma_i \in \{-1, 1\}$. 

Second, we make a mean-field nearest-neighbor approximation. 
That is, we assume strength of pairwise interactions is negligible for 
non-adjacent pairs of atoms. 
We also assume the non-negligible $J_{ij}$'s are all equal to some constant $J$, 
that is, the material is organized in some regular, symmetric lattice. 

Coalescing constants, the Hamiltonian reduces nicely to:
\begin{equation}\label{eq:isingE}
    E(\vec{\sigma}) = -J\sum_i^n \sum_{j \in adj(i)} \sigma_i \sigma_j - \sum_i^n \sigma_i B_i
\end{equation}
The Boltzmann distribution then gives the probability of some microstate of 
spins $\vec{\sigma} = (\sigma_1, \dots, \sigma_k)$ at inverse temperature $\beta$:
\begin{equation} \label{eq:boltzmann}
    P(\vec{\sigma}) = \frac{1}{Z}e^{-\beta E(\vec{\sigma})} 
\end{equation}
The physical theory of the Ising model has now been developed and we can turn to 
the task of inference on a PGM. 
%
%
%
%
%
\section{Inference on Ising Models}
%
%
%
%
%
In many cases, the Ising model of ferromagnetism is used to produce physical 
measurables like energy and magnetization. 
For example, Ernst Ising's original inquiry asked if the Ising model was capable 
of demonstrating phase transitions. 

Now, these measurables require marginal distributions in order to compute. 
For example, the quantities we are most interested in, the energy 
\ref{eq:isingE} and the magnetization $\mathbb{E}[\sigma]$, both require 
explicit microstates to compute. 
We will now examine two approaches to estimating these marginals—Markov Chain 
Monte Carlo and belief propagation. 
%
%
%
%
%
\subsection{Markov Chain Monte Carlo}
%
%
%
%
%
Monte Carlo algorithms produce better and better estimates by repeated sampling. 
When the true distribution is not immediately available for sampling, 
constructing an appropriate Markov Chain is one way to approximate it. 

In the context of the Ising model, the Markov transitions available from some 
microstate of spins $\vec{\sigma}$ are those created by flipping any spin 
$\sigma_i$ in $\vec{\sigma}$. 
We can sample from these available transitions, then, by choosing a random 
$\sigma_i$ to flip. 
Since we want to move towards the true, equilibrium distribution, we only 
transition to the new sampled microstate $\vec{\sigma}'$ if 
$P(\vec{\sigma}') > P(\vec{\sigma})$. 
Using the formula for the Boltzmann distribution, this criterion is equivalent 
to $E(\vec{\sigma}') < E(\vec{\sigma})$. 

In practice, we will compute the change in energy 
$\Delta E = E(\vec{\sigma}') - E(\vec{\sigma})$, which has a nice form:
\[
    \Delta E = E(\vec{\sigma}') - E(\vec{\sigma}) = 2 \sigma_i (J \sum_{j \in adj(i)} \sigma_j + B_i)
\]
If $\Delta E < 0$, we transition states. 
If $\Delta E \geq 0$, we will transition microstates with probability defined by 
the Boltzmann factor:
\[
    \frac{P(\vec{\sigma}')}{P(\vec{\sigma})}
    = e^{-\beta \Delta E} 
\]
Physically, this nicely models the phenomena of spin flips caused by field 
fluctuations. 

We implemented this process in Python on square lattices of spins. 
For our sampling process, we iterated through all spins in random order, at each 
step evaluating the transition defined by flipping that spin. 
We believe this is a good trade off between performance and an accurate 
simulation of the physical equilibration. 

\begin{figure}[h]
    \begin{subfigure}{\textwidth}
        \includegraphics[width=\textwidth]{report_mcmc_gaussian}
        \centering
        \caption{\textit{
            Spin lattice sample microstates from various points in the sampling 
            process
        }}
        \label{fig:mcmc_gaussian_a}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \includegraphics[width=\textwidth]{report_mcmc_gaussian_measurables}
        \centering
        \caption{\textit{Measurables of samples during the sampling process}}
        \label{fig:mcmc_gaussian_b}
    \end{subfigure}
    \centering
    \caption{\textit{
        MCMC quenching of an initial randomly generated $256 \times 256$ spin 
        lattice sample with $J = 0.5$, $\beta = 10$, and a centered Gaussian 
        magnetic field
        $B_{ij} = 0.01 \exp [-\frac{1}{1024}((i-128)^2 + (j-128)^2)]$. 
    }}
    \label{fig:mcmc_gaussian}
\end{figure}

In the MCMC quenching simulation conducted in \ref{fig:mcmc_gaussian}, we 
observe several expected phenomena. 
The pairwise interactions are exhibited by the clusters of aligned spins which 
form and coalesce as the number of steps grows. 
The prescence of the magnetic field also seems to magnetize the entire lattice, 
as indicated both by the convergence on full spin alignment in 
\ref{fig:mcmc_gaussian_a}, and the increasing magnetization measured in 
\ref{fig:mcmc_gaussian_b}. 

\begin{figure}
    \includegraphics[width=\textwidth]{report_mcmc_phase}
    \caption{\textit{
        $100 \times 100$ spin lattice run with $J=0.5$, $\vec{B} = 0$ and 
        $T \in \{0.01, 0.02, \dots, 2\}$. 
        Note that $\beta = \frac{1}{T}$. 
        An hand-estimated Curie temperature is marked in red. 
    }}
    \label{fig:mcmc_phase}
\end{figure}

Using this MCMC inference scheme, we ran an experiment (Figure 
\ref{fig:mcmc_phase}) simulating the phase transitions expected of the 2D 
Ising model \cite{peirerls1936}. 

As seen, the energy and magnetization of the lattice both degenerate to zero 
as the temperature increases. 
The physical interpretation of this is that, at high temperatures, the field 
fluctuations which cause spin flips become more and more likely, and thus 
the alignment of spins is constantly being disrupted. 
It has been shown analytically that the Ising model in 2D exhibits this 
phenomenon of phase transitions. 
This simulation reinforces that claim. 
We can also identify what is called the Curie temperature, the point at which 
the phase transition occurs. 
%
%
%
%
%
\subsection{Belief Propagation}
%
%
%
%
%
Message-passing belief propagation algorithms are another way to estimate true 
marginals for general graphical models. 
They are derived from the theory of variable elimination, but are applicable to 
general graphs which may include cycles. 

Belief propagation is especially simple to implement on Ising models. 
This follows from the Boltzmann distribution that defines the joint distribution:
\begin{equation}\label{isingJoint}
    \tilde{P}(\vec{\sigma}) = \exp \Bigr [\beta J\sum_i \sum_{j \in adj(i)} \sigma_i \sigma_j + \beta \sum_i \sigma_i B_i \Bigr ]
\end{equation}
It factorizes nicely over the graph representing the Ising lattice:
\begin{equation}\label{gibbsFactorization}
    \tilde{P}(\vec{\sigma}) = \prod_i \exp \Bigr [\beta \sigma_i B_i \Bigr ] \prod_{j \in adj(i)} \exp \Bigr [ \beta J \sigma_i \sigma_j \Bigr ]
\end{equation}
This defines a factor graph of node and edge potentials representing the unary 
and pairwise potentials, respectively:
\[
    \phi_i = \exp \Bigr [\beta \sigma_i B_i \Bigr ]
    \qquad \qquad
    \phi_{ij} = \exp \Bigr [ \beta J \sigma_i \sigma_j \Bigr ]
\]
We ran a belief propagation routine on the factor graph according to the update 
scheme in Algorithm \ref{alg:bp}. 

\begin{algorithm}
    \caption{Belief propagation calibration step on factor graph}
    \begin{algorithmic}[1]
        \Statex \underline{\textbf{Input}}
        \Statex \quad Factor graph: $G(V = \{v_i\}_i \cup \{v_{ij}\}_{i, j}, E)$
        \Statex \quad Unary Factors: $\{\phi_i(\sigma_i)\}_i$
        \Statex \quad Pairwise Factors: $\{\phi_{ij}(\sigma_i, \sigma_j)\}_{i, j}$

        \Statex \underline{\textbf{Output:}}
        \Statex \quad Unary beliefs: $\{\beta_i(\sigma_i)\}_i$
        \Statex \quad Pairwise beliefs: $\{\beta_{ij}(\sigma_i, \sigma_j)\}_{i, j}$
        
        \Statex // Pass messages from unary nodes
        \For{unary $v_i \in V$}
        \For{pairwise $v_{ij} \in Nb(v_i)$}
        \State $\delta[v_i \rightarrow v_{ij}] = \phi_i \prod_{k \neq j} \delta[v_{ik} \rightarrow v_i]$ \quad // Nothing to sum!
        \EndFor
        \EndFor

        \Statex // Pass messages from pairwise nodes
        \For{pairwise $v_{ij} \in V$}
        \For{$\{v_i, v_j\} = Nb(v_{ij})$}
        \State $\delta[v_{ij} \rightarrow v_i] = \sum_{v_j} \phi_{ij} \delta[v_j \rightarrow v_{ij}]$
        \State $\delta[v_{ij} \rightarrow v_j] = \sum_{v_i} \phi_{ij} \delta[v_i \rightarrow v_{ij}]$
        \EndFor
        \EndFor

        \Statex // Compute beliefs
        \For{unary $v_i \in V$}
        \State $\beta_i = \phi_i \prod_{j} \delta[v_{ij} \rightarrow v_i]$
        \EndFor
        \For{pairwise $v_{ij} \in V$}
        \State $\beta_{ij} = \phi_{ij} \delta[v_i \rightarrow v_{ij}] \delta[v_j \rightarrow v_{ij}]$
        \EndFor
        \State \textbf{return } $\{\beta_i\}_i, \{\beta_{ij}\}_{ij}$
    \end{algorithmic}
    \label{alg:bp}
\end{algorithm}

\begin{figure}
    \begin{subfigure}{\textwidth}
        \includegraphics[width=\textwidth]{report_bp_gaussian}
        \centering
        \caption{\textit{
            Spin lattice marginal beliefs at various points in the calibration 
            process. 
        }}
        \label{fig:bp_gaussian_a}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \includegraphics[width=\textwidth]{report_bp_gaussian_measurables}
        \centering
        \caption{\textit{Mean value of measurables taken from samples from marginals}}
        \label{fig:bp_gaussian_b}
    \end{subfigure}
    \centering
    \caption{\textit{
        Belief propagation to estimate marginal distributions of spin lattice 
        with $J = 0.5$, $\beta = 10$, and a centered Gaussian 
        magnetic field
        $B_{ij} = 0.01 \exp [-\frac{1}{1024}((i-128)^2 + (j-128)^2)]$. 
        We run belief propagation until the difference between messages falls 
        below $\epsilon = 0.001$. 
    }}
    \label{fig:bp_gaussian}
\end{figure}

We tested this on the same system from \ref{fig:mcmc_gaussian}. 
Algorithm \ref{alg:bp} is run repeatedly until the maximum change in the 
messages falls below some $\epsilon$. 
We also take pseudo-measurements by sampling from the marginal beliefs at 
each step and computing their mean energy and magnetization. 

After calibration, the approximate marginals visualized in Figure
\ref{fig:bp_gaussian_a} reflect what we would expect, full system spin alignment 
by the magnetic field. 
The trend in the magnetization is also observable in the plots of sample 
pseudo-measurables, which reflect the appropriate minimization of energy. 

\subsection{Comparing MCMC and belief propagation}

When deciding which paradigm of inference to use for the Ising model, there are 
a number of considerations with respect to speed, output, interpretability, etc. 
Here, we will discuss these factors with respect to MCMC and belief propagation. 

MCMC is useful because it provides easy access to valid microstate samples 
throughout the estimation process. 
Importantly, this makes the computation of meaningful measurables easy and 
intuitive. 
This is a direct consequence of the simulatory nature of MCMC, which emulates, 
as closely as possible, the underlying physical processes. 

Unfortunately, MCMC is slow, and very difficult to parallelize. 
This slowness is compounded by its tendency to follow unoptimal branches in the 
search tree and get stuck in local minima. 

Belief propagation, on the other hand, is useful because of the immediate access 
to highly-accurate marginal beliefs, upon calibration. 
Belief propagation is also fast. 
It is highly parallelizable, especially on factor graphs, and inherently 
bypasses many of the search tree inefficiencies that Monte Carlo methods 
struggle with. 

The downsides of belief propagation lie in the poor interpretability of 
intermediate steps. 
Since the marginal beliefs do not necessarily agree with each other until 
they are fully calibrated, they do not correspond to physically meaningful 
microstates when sampled. 
As a consequence, it is difficult to take measurements of the system. 
Deriving macrostate properties from the partition function may be one solution, 
but we leave that problem to future studies. 

For most physical use cases, we conclude that MCMC is the better approach to 
inference. 
The interpretability and ease of measurement is extremely valuable, and lends 
itself to the precise simulation and analysis of physical phenomena such as 
phase transition. 
%
%
%
%
%
\section{Conclusions}
%
%
%
%
%
In this paper, we revisited the Ising model, an important probabilistic model of 
ferromagnetism, to pay homage and exercise modern computational methods. 
We discuss the physical theory and used it to motivate the PGMs methods applied. 
We evaluated two approaches, Markov Chain Monte Carlo and belief propagation, 
for marginal inference on the Ising model, and discussed their respective 
strengths and weaknesses. 

We concluded MCMC was the better approach in the context of the Ising model 
because of its strong interpretability and ease of measurement. 
These strengths enables us to demonstrate the phenomenon of magnetic phase 
transitions in a highly emulative way. 
However, for faster inference that is interested in the true equilibrium beliefs 
of the system, belief propagation may prove more effective. 

For the future, there is much to explore both with respect to the problem of 
magnetism as well as the Ising model. 
For example, deriving beliefs about macrostate measurables, which might involve 
the partition function, is an interesting and challenging task that might lessen 
the reliance on MCMC. 
There is also the problem of better simulation and analysis of phase transition 
phenomena, which might necessitate the adaptation of renormalization group 
theory to machine learning. 

We conclude by affirming the power and usefulness of the Ising model in the 
context of physical as well as abstract probabilistic modeling. 
Within its rich theory, we can intuit much about modern probabilistic 
reasoning and its connection with the study of physical ensembles. 
Additionally, in the rich history of the model, in the story leading from 
ferromagnetism to deep neural networks, we can find solace in the fact that 
today's crude toy models might inspire tommorow's transhuman super-intelligence. 
%
%
%
%
%
\section{Supplementary Material}
%
%
%
%
%
Our implementations of the above algorithms, using Python and the NumPy package, 
can be found at \url{https://github.com/jshe2304/ttic31180}. 

\bibliographystyle{unsrt}
\bibliography{references}

\end{document}