\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[preprint]{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{algpseudocode}
\usepackage{algorithm}

\title{Theory and Methods for the Ferromagnetic Ising Model}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{
    Jay Shen \\
    Department of Physics \\
    University of Chicago\\
    Chicago, IL 60637 \\
    \texttt{jshe@uchicago.edu} \\
    \And
    Mark Lee \\
    Department of Statistics \\
    University of Chicago\\
    Chicago, IL 60637 \\
    \texttt{markyl@uchicago.edu} \\
}


\begin{document}

\graphicspath{ {../graphics} }

\maketitle

\begin{abstract}

The Ising model is a historically important problem in statistical mechanics. 
Although it was originally proposed as a crude approximation of magnetic 
phenomena, it furnished statistical mechanics, and later probabilistic science 
as whole, with a new paradigm of graph-based modeling that would prove 
influential.
Especially in the distilled, theoretical study of probabilistic graphical 
models (PGMs), which today is its own field, many methods developed for Ising 
models and spin-glasses—as well as the rich physical vocabulary of energies, 
entropy, and partition functions—have been repurposed and reinterpreted for 
application towards a wide variety of problems. 

In this paper, we pay homage to the Ising model by applying modern computational 
solutions towards the original problem of magnetism. 
We discuss the theory of the Ising model from a physical perspective, and 
examine its correspondence with PGMs theory. 
We then evaluate two approaches to inference—Markov Chain Monte Carlo and
belief propagation. 
Finally, we show the success of the Ising model at modeling some physical 
phenomena, and discuss its limitations. 
%
%
%
%
%
\end{abstract}
%
%
%
%
%
\section{Theory of the Ising Model}

The prevailing physical theory explains magnetism as a consequence of the intrinsic 
spin of particles. 
This spin produces a magnetic dipole moment $\vec{\mu}$ proportional to the 
spin $\sigma$:
\[\vec{\mu} = \mu \sigma\]
If an external magnetic field $\vec{B}$ is present, the potential energy is 
given by the classical formula:
\[U_B = - \vec{\mu} \cdot \vec{B}\]
Handily, this also absorbs the splitting of atomic energy levels due to quantum 
mechanical phenomena. 

There also exists a pairwise interaction between particles due to the mutual 
effects of their magnetic moments.
These exchange effects have strength defined by constants $J_{ij}$, 
The potential energy of these pairwise interactions is then:
\[U_{ij} = - J_{ij} \vec{\mu_i} \cdot \vec{\mu_j}\]
Here, we ignore quantum mechanical spin-spin coupling and exclusion energies. 

Note that both potential energies are minimized when the moments align with 
$\vec{B}$ and with each other. 
This corresponds to the theory of ferromagnetism as a systematic alignment of 
spins. 

Now, consider a collection of particles $\vec{\mu_i}$ in the prescence of an 
external magnetic field $\vec{B}$. 
The Hamiltonian, which specifies the total energy of the system, is defined by 
the sum of all pairwise and unary energies:
\begin{equation}\label{exactE}
    E(\vec{\mu}) = -\frac{1}{2}\sum_i \sum_j J_{ij} \vec{\mu_i} \cdot \vec{\mu_j} - \sum_i \vec{\mu_i} \cdot \vec{B}
\end{equation}
In most cases, working with this Hamiltonian is intractable. 
Luckily, we can simplify it by making several strategic assumptions. 

First, in the context of an atomic lattice, all relevant particles are fermionic 
and have spin-$\frac{1}{2}$. 
We assume that all particles within an atom have identical spin, so the spin of 
each atom as a whole is fermionic. 
Then the magnetic moments simplify to $\vec{\mu_i} = \mu \sigma_i$, where 
$\sigma_i \in \{-1, 1\}$ is the atomic spin. 

Second, we make a mean-field nearest-neighbor approximation so that the strength 
of pairwise interactions are negligible for non-adjacent pairs. 
We also assume all non-negligible $J_{ij}$'s are equivalent, that is that the 
material is organized in a regular lattice. 

Coalescing constants, the Hamiltonian reduces nicely to:
\begin{equation}\label{isingE}
    E(\vec{\sigma}) = -J\sum_i \sum_{j \in adj(i)} \sigma_i \sigma_j - \sum_i \sigma_i B_i
\end{equation}
The Boltzmann distribution gives a probability of some microstate $\vec{\sigma}$ 
at inverse temperature $\beta$:
\begin{equation} \label{boltzmann}
    P(\vec{\sigma}) = \frac{1}{Z}e^{-\beta E(\vec{\sigma})} 
\end{equation}
The physical theory of the Ising model has now been developed and we can turn to 
the purely abstract task of inference on a PGM. 
%
%
%
%
%
\section{Inference on Ising Models}
%
%
%
%
%
In many cases, the Ising model of ferromagnetism is used to produce physical 
measurables like energy and magnetization. 
For example, Ernst Ising's original inquiry asked if the Ising model was capable 
of demonstrating phase transitions—changes in the magnetic state due to external 
conditions. 

Now, these measurables require marginal distributions in order to compute, for 
example, the unary and pairwise energies. 
We will now examine two approaches to computing these marginals—Markov Chain 
Monte Carlo and belief propagation. 
%
%
%
%
%
\subsection{Markov Chain Monte Carlo}
%
%
%
%
%
Monte Carlo algorithms produce better and better estimates by repeated sampling 
from the true distribution. 
When the true distribution is not immediately available for sampling, employing 
Markov Chain Monte Carlo defines an approximate distribution that hopefully, 
during sampling, approaching the true distribution. 

In the context of the Ising model, the Markov states proceeding from some 
microstate of spins $\vec{\sigma}$ are created by flipping any spin $\sigma_i$ in 
$\vec{\sigma}$. 
We can sample from these adjacent microstates by choosing a random $\sigma_i$ to 
flip. 
Since we want to move towards the true, equilibrium distribution, we only 
transition to the new microstate $\vec{\sigma}'$ if 
$P(\vec{\sigma}') > P(\vec{\sigma})$. 
Using the formula for the Boltzmann distribution, this criterion is equivalent 
to $E(\vec{\sigma}') < E(\vec{\sigma})$. 

In practice, we will compute the change in energy 
$\Delta E = E(\vec{\sigma}') - E(\vec{\sigma})$, which has a nice form:
\[
    \Delta E = E(\vec{\sigma}') - E(\vec{\sigma}) = 2 \sigma_i (J \sum_{j \in adj(i)} \sigma_j + B_i)
\]
If $\Delta E < 0$, we transition states. 
If $\Delta E \geq 0$, we will transition microstates with probability defined by 
the Boltzmann factor:
\[
    \frac{P(\vec{\sigma}')}{P(\vec{\sigma})}
    = e^{-\beta \Delta E} 
\]
This nicely models the phenomena of spin flips caused by field fluctuations. 

We implemented this process in Python on square lattices of spins. 
For our sampling process, we iterated through all spins in random order, at each 
step evaluating the transition defined by flipping the present spin. 
We believe this best simulates the actual equilibration process. 

\begin{figure}
    \begin{subfigure}{\textwidth}
        \includegraphics[width=\textwidth]{report_mcmc_gaussian}
        \centering
        \caption{\textit{
            Spin lattice sample microstates from various points in the sampling 
            process
        }}
        \label{fig:mcmc_gaussian_a}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \includegraphics[width=\textwidth]{report_mcmc_gaussian_measurables}
        \centering
        \caption{\textit{Measurables of samples during the sampling process}}
        \label{fig:mcmc_gaussian_b}
    \end{subfigure}
    \centering
    \caption{\textit{
        MCMC quenching of an initial randomly generated $256 \times 256$ spin 
        lattice sample with $J = 0.5$, $\beta = 10$, and a centered Gaussian 
        magnetic field
        $B_{ij} = 0.01 \exp [-\frac{1}{1024}((i-128)^2 + (j-128)^2)]$. 
    }}
    \label{fig:mcmc_gaussian}
\end{figure}
In the MCMC quenching simulation conducted in \ref{fig:mcmc_gaussian}, we 
observe several expected phenomena. 
The pairwise interactions are clearly exhibited by the clusters of aligned spins 
which form and coalesce as the number of steps grows. 
The prescence of the magnetic field also seems to magnetize the entire lattice, 
as indicated both by the convergence on full spin alignment in 
\ref{fig:mcmc_gaussian_a}, and the increasing magnetization measured in 
\ref{fig:mcmc_gaussian_b}. 

Note that computing the measurables for \ref{fig:mcmc_gaussian_a} and 
\ref{fig:mcmc_gaussian_b} is easy given the explicit spin lattice microstates 
provided by the sampling process. 
The magnetization is simply the average spin. 
The energy is computed using \ref{isingE}. 

\begin{figure}
    \includegraphics[width=\textwidth]{mcmc_phase}
    \label{fig:mcmc_phase}
\end{figure}

Using this MCMC inference scheme, we ran an experiment (Figure 
\ref{fig:mcmc_gaussian}) simulating the phase transitions expected of the 2D 
Ising model. 


%
%
%
%
%
\subsection{Belief Propagation}
%
%
%
%
%
Message-passing belief propagation algorithms are another way to estimate true 
marginals for general graphical models. 
They are derived from the theory of variable elimination, but are applicable to 
general graphs which may include cycles. 

It turns out that belief propagation is especially effective and simple to 
implement on Ising models. 
This follows from the Boltzmann distribution that defines the joint distribution 
of the Ising model:
\begin{equation}\label{isingJoint}
    \tilde{P}(\vec{\sigma}) = \exp \Bigr [\beta J\sum_i \sum_{j \in adj(i)} \sigma_i \sigma_j + \beta \sum_i \sigma_i B_i \Bigr ]
\end{equation}
It factorizes nicely over the Ising lattice:
\begin{equation}\label{gibbsFactorization}
    \tilde{P}(\vec{\sigma}) = \prod_i \exp \Bigr [\beta \sigma_i B_i \Bigr ] \prod_{j \in adj(i)} \exp \Bigr [ \beta J \sigma_i \sigma_j \Bigr ]
\end{equation}
This defines a factor graph of node and edge potentials representing the unary 
and pairwise potentials, respectively:
\[
    \phi_i = \exp \Bigr [\beta \sigma_i B_i \Bigr ]
    \qquad \qquad
    \phi_{ij} = \exp \Bigr [ \beta J \sigma_i \sigma_j \Bigr ]
\]
We then run a belief propagation routine on the factor graph according to the 
update scheme in Algorithm \ref{alg:bp}. 

\begin{algorithm}
    \caption{Belief propagation calibration step on factor graph}
    \begin{algorithmic}[1]
        \Statex \underline{\textbf{Input}}
        \Statex \quad Factor graph: $G(V = \{v_i\}_i \cup \{v_{ij}\}_{i, j}, E)$
        \Statex \quad Unary Factors: $\{\phi_i(\sigma_i)\}_i$
        \Statex \quad Pairwise Factors: $\{\phi_{ij}(\sigma_i, \sigma_j)\}_{i, j}$

        \Statex \underline{\textbf{Output:}}
        \Statex \quad Unary beliefs: $\{\beta_i(\sigma_i)\}_i$
        \Statex \quad Pairwise beliefs: $\{\beta_{ij}(\sigma_i, \sigma_j)\}_{i, j}$
        
        \Statex // Pass messages from unary nodes
        \For{unary $v_i \in V$}
        \For{pairwise $v_{ij} \in Nb(v_i)$}
        \State $\delta[v_i \rightarrow v_{ij}] = \phi_i \prod_{k \neq j} \delta[v_{ik} \rightarrow v_i]$ \quad // Nothing to sum!
        \EndFor
        \EndFor

        \Statex // Pass messages from pairwise nodes
        \For{pairwise $v_{ij} \in V$}
        \For{$\{v_i, v_j\} = Nb(v_{ij})$}
        \State $\delta[v_{ij} \rightarrow v_i] = \sum_{v_j} \phi_{ij} \delta[v_j \rightarrow v_{ij}]$
        \State $\delta[v_{ij} \rightarrow v_j] = \sum_{v_i} \phi_{ij} \delta[v_i \rightarrow v_{ij}]$
        \EndFor
        \EndFor

        \Statex // Compute beliefs
        \For{unary $v_i \in V$}
        \State $\beta_i = \phi_i \prod_{j} \delta[v_{ij} \rightarrow v_i]$
        \EndFor
        \For{pairwise $v_{ij} \in V$}
        \State $\beta_{ij} = \phi_{ij} \delta[v_i \rightarrow v_{ij}] \delta[v_j \rightarrow v_{ij}]$
        \EndFor
        \State \textbf{return } $\{\beta_i\}_i, \{\beta_{ij}\}_{ij}$
    \end{algorithmic}
    \label{alg:bp}
\end{algorithm}

\begin{figure}
    \begin{subfigure}{\textwidth}
        \includegraphics[width=\textwidth]{report_bp_gaussian}
        \centering
        \caption{\textit{
            Spin lattice marginal beliefs at various points in the calibration 
            process. 
        }}
        \label{fig:bp_gaussian_a}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \includegraphics[width=\textwidth]{report_bp_gaussian_measurables}
        \centering
        \caption{\textit{Mean value of measurables taken from samples from marginals}}
        \label{fig:bp_gaussian_b}
    \end{subfigure}
    \centering
    \caption{\textit{
        Belief propagation to estimate marginal distributions of spin lattice 
        with $J = 0.5$, $\beta = 10$, and a centered Gaussian 
        magnetic field
        $B_{ij} = 0.01 \exp [-\frac{1}{1024}((i-128)^2 + (j-128)^2)]$. 
        We run belief propagation until the difference between messages falls 
        below $\epsilon = 0.001$. 
    }}
    \label{fig:bp_gaussian}
\end{figure}
In Figure \ref{fig:bp_gaussian}, we repeatedly run Algorithm \ref{alg:bp} on the 
same system from Figure \ref{fig:mcmc_gaussian}, until the maximum change in the 
messages falls below some $\epsilon$. 
We also compute some measurables by taking samples from the marginal beliefs at 
each step and computing their mean energy and magnetization. 

After calibration, the approximate marginals visualized in Figure
\ref{fig:bp_gaussian_a} reflect what we would expect, full system spin alignment 
by the magnetic field. 
The trend in the magnetization is also observable in the plots of sample 
measurables which also reflects the appropriate minimization of energy. 

\subsection{Comparing MCMC and belief propagation}

There are a number of considerations to keep in mind when deciding how to do 
inference on Ising models. 

The benefits of MCMC inference lie in the easy access to valid microstate 
samples throughout the estimation process, which makes the computation of 
meaningful measurables easy. 
This is a direct consequence of the simulatory nature of MCMC which emulates, as 
closely as possible, the physical processes that underlie our desired marginal 
distributions. 

Unfortunately, MCMC is slow, and very difficult to parallelize. 
This slowness is compounded by its tendency to follow unoptimal branches in the 
search tree and get stuck in local minima. 

Belief propagation has its upsides in the immediate access to highly-accurate 
marginal beliefs upon calibration. 
Belief propagation is also fast. 
It is highly parallelizable, especially on factor graphs, and inherently 
bypasses many of the search tree inefficiencies that Monte Carlo methods 
struggle with. 

The downsides of belief propagation lie in the poor interpretability of 
intermediate steps. 
Since the marginal beliefs do not necessarily agree with each other before 
calibration, they do not correspond to physically meaningful microstates upon 
sampling. 
As a consequence, it is difficult to take measurements of the system without 
sampling. 
Computing the partition function derivatives may be one solution, but we leave 
that problem to future studies. 

For most physical use cases, we conclude that MCMC is the better approach to 
inference. 
The interpretability and ease of measurements when dealing with explicit 
microstates is extremely valuable, especially in the context of magnetism. 
As we showed, simulating and analyzing physical phenomena such as phase 
transitions are straightforward and can be very precise. 

\section{Conclusions}

In this paper we have 

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}